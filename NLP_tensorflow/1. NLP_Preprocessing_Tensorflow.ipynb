{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75448ff2",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "- Process of representing words in a way that a computer can process them on later training on a NN that can understand their meaning.\n",
    "\n",
    "### Concept\n",
    "\n",
    "- ASCII can be used to represent words\n",
    "#### Example\n",
    "1. LISTEN = 083, 073, 076, 069, 078, 084\n",
    "2. SILENT = 076, 073, 083, 084, 069, 078\n",
    "\n",
    "- **`Disadvantage`** Both words when presented in different orders have same ascii values\n",
    "\n",
    "- Hence instead of encoding Letters,**encode words**.\n",
    "#### Example\n",
    "- I Love my Dog : 001, 002, 003, `004`\n",
    "- I Love my Cat : 001, 002, 003, `005`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd87cc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e642b517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Representing array as python array of strings\n",
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'you love my dog!'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcb64c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
     ]
    }
   ],
   "source": [
    "#creating instance of tokenizer object.\n",
    "#keeping just most frequent 100words\n",
    "tokenizer = Tokenizer(num_words=100) #num_words parameter is the maximum words to keep\n",
    "\n",
    "#tokenizer move through all text and fit itself to them\n",
    "tokenizer.fit_on_texts(sentences)  \n",
    "\n",
    "#Full list of words is available as the word index property\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc358a5c",
   "metadata": {},
   "source": [
    "## Sequencing - Turning sentences into data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fae10bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences1 = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'you love my dog!',\n",
    "    'Do you think my dog is amazing?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "572cba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=100)\n",
    "\n",
    "#we have set of sentences that we will use for training a NN\n",
    "tokenizer.fit_on_texts(sentences1)\n",
    "\n",
    "#The tokenizer gets the word index from the sentences used for training\n",
    "#and create sequences\n",
    "word_index=tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7adae6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates sequences of tokens representing each sentences\n",
    "\n",
    "sequences=tokenizer.texts_to_sequences(sentences1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82f38161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n",
      "['I love my dog', 'I love my cat', 'you love my dog!', 'Do you think my dog is amazing?']\n"
     ]
    }
   ],
   "source": [
    "print(word_index)\n",
    "print(sentences1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44d23ba",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "Q1 What happens when the NN needs to classify texts, but there are words in the text that it has never seen before?\n",
    "> This confuses the tokenizer! How to handle this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9929794",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    'i really love my dog',\n",
    "    'my dog loves my manatee'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33b97c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 2, 1, 3], [1, 3, 1]]\n"
     ]
    }
   ],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05b6c78",
   "metadata": {},
   "source": [
    "- since `manatee`, `really` etc words are not present in word_index, because they were not in the initial set of data\n",
    "\n",
    "#### Result\n",
    "- 5 word first sentence ends up as 4,2,1,3 as a 4 word sequence OR second sentence ends up as a 1,3,1 as the corpus used to build it didnt contain that word i.e **`loves, manatee and really`** are not in the word index\n",
    "\n",
    "\n",
    "####  Conclusion\n",
    "- We thus requires a big word index to handle sentences that are not in the training set.\n",
    "\n",
    "### Solution\n",
    "- In order not to loose the length of the sequence, use **`oov_token` property** setting for words not expected to be in the corpus,\n",
    "- Tokenizer will create token for that and then replace words that it doesn't recognize with the out of Vocabulary token instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaef3a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences1)\n",
    "word_index = tokenizer.word_index\n",
    "seqences = tokenizer.texts_to_sequences(sentences1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25fdec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    'i really love my dog',\n",
    "    'my dog loves my manatee'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "016e3efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
      "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"
     ]
    }
   ],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(word_index)\n",
    "print(test_seq)\n",
    "\n",
    "#Result: Sentence will not loose length\n",
    "#token 1 is provided to not recognized words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bed4fe",
   "metadata": {},
   "source": [
    "- We still lost meaning of the sentence but we atleast got the correct length.\n",
    "\n",
    "- But while it helps maintain the sequence length to be the same length as the sentence, we might need to train a NN, how it can handle sentences of different lengths?\n",
    "\n",
    "- Images are of same size, but sentences are of different lengths!!\n",
    "\n",
    "**1. The solution is by using RaggedTensor**\n",
    "\n",
    "**Simpler Solution is Padding**\n",
    "\n",
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "469b8b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fc94699",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences1 = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'you love my dog!',\n",
    "    'Do you think my dog is amazing?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bfc497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences1)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "seqences = tokenizer.texts_to_sequences(sentences1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68596043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
      "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n",
      "[[ 0  0  0  4  2  1  3]\n",
      " [ 0  0  0  4  2  1  6]\n",
      " [ 0  0  0  5  2  1  3]\n",
      " [ 7  5  8  1  3  9 10]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(sequences)\n",
    "print(word_index)\n",
    "print(sequences)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6028f2b",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "\n",
    "- Since longest sentence have 7 words in it hence additional 0's are added in remaining sentence\n",
    "- We can also assigned the position to pre or post, in this way it ensure that all have equally sized sequences by paddng them with 0's at the front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e47cf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  2  1  3  0  0  0]\n",
      " [ 4  2  1  6  0  0  0]\n",
      " [ 5  2  1  3  0  0  0]\n",
      " [ 7  5  8  1  3  9 10]]\n"
     ]
    }
   ],
   "source": [
    "#to pad sequence with 0's at post position\n",
    "\n",
    "padded = pad_sequences(sequences, padding='post')\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abc9ab05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  2  1  3  0]\n",
      " [ 4  2  1  6  0]\n",
      " [ 5  2  1  3  0]\n",
      " [ 8  1  3  9 10]]\n"
     ]
    }
   ],
   "source": [
    "#if we dont require the padded sentence length to be equal to\n",
    "#longest sentence we can use maxlen parameter\n",
    "\n",
    "padded = pad_sequences(sequences,padding='post', maxlen=5)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47275eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 4 2 1 3]\n",
      " [0 4 2 1 6]\n",
      " [0 5 2 1 3]\n",
      " [7 5 8 1 3]]\n"
     ]
    }
   ],
   "source": [
    "#we can decide from where words should be \n",
    "\n",
    "padded = pad_sequences(sequences,truncating='post', maxlen=5)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe89d32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
